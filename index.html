<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yue Liao | Academic Homepage</title>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --text-color: #34495e;
            --bg-light: #f9f9f9;
            --shadow: 0 2px 15px rgba(0,0,0,0.1);
        }

        body {
            font-family: 'Lato', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 20px;
            background-color: white;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .header {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            align-items: center;
            padding: 40px 0;
            border-bottom: 2px solid #eee;
        }

        .profile-img {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            object-fit: cover;
            box-shadow: var(--shadow);
        }

        .name {
            font-size: 2.5em;
            margin: 0;
            color: var(--primary-color);
        }

        .contact-links {
            margin-top: 15px;
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }

        .contact-links a {
            color: var(--secondary-color);
            transition: color 0.3s;
            text-decoration: none;
        }

        .contact-links a:hover {
            color: var(--accent-color);
        }

        .section {
            margin: 40px 0;
            padding: 30px;
            background: white;
            border-radius: 10px;
            box-shadow: var(--shadow);
        }

        .section-heading {
            font-size: 1.8em;
            color: var(--primary-color);
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
            margin: 0 0 25px 0;
        }

        .news-list {
            list-style: none;
            padding: 0;
        }

        .news-item {
            padding: 15px 0;
            border-bottom: 1px solid #eee;
            display: flex;
            align-items: baseline;
        }

        .news-date {
            color: var(--accent-color);
            min-width: 100px;
            font-weight: 700;
        }

        .publication {
            padding: 20px;
            margin: 15px 0;
            background: var(--bg-light);
            border-radius: 8px;
            transition: transform 0.2s;
        }

        .publication:hover {
            transform: translateY(-3px);
        }

        .pub-title {
            font-size: 1.1em;
            color: var(--primary-color);
            margin: 0 0 10px 0;
        }

        .pub-authors {
            color: #666;
            margin: 5px 0;
        }

        .pub-venue {
            color: var(--secondary-color);
            font-weight: 700;
        }

        .pub-links {
            margin-top: 10px;
        }

        .pub-links a {
            margin-right: 15px;
            color: var(--accent-color);
            text-decoration: none;
        }

        .list {
            list-style: none;
            padding: 0;
        }

        .list li {
            padding: 8px 0;
            border-bottom: 1px solid #eee;
        }

        @media (max-width: 768px) {
            .header {
                flex-direction: column;
                text-align: center;
            }
            
            .contact-links {
                justify-content: center;
            }
            
            .section {
                padding: 20px;
            }
            
            .news-date {
                min-width: 80px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="header-text">
                <h1 class="name">Yue Liao <span style="font-family: 'KaiTi', sans-serif; font-size: 0.8em;">廖越</span></h1>
                <p>Post-Doctoral Fellow, MMLab<br>The Chinese University of Hong Kong</p>
                <div class="contact-links">
                    <a href="mailto:liaoyue.ai@gmail.com"><i class="fas fa-envelope"></i> Email</a>
                    <a href="https://scholar.google.com/citations?user=mIt-3fEAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Scholar</a>
                    <a href="https://github.com/YueLiao"><i class="fab fa-github"></i> GitHub</a>
                </div>
            </div>
            <img src="images/liaoyue.jpg" alt="Yue Liao" class="profile-img">
        </header>

        <section class="section">
            <h2 class="section-heading">Research Interests</h2>
            <p>Computer Vision, Deep Learning, Vision and Language</p>
        </section>

        <section class="section">
            <h2 class="section-heading">Latest News</h2>
            <ul class="news-list">
                <li class="news-item">
                    <span class="news-date">2025/01</span>
                    <span>Three papers accepted to ICLR 2025</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2022/06</span>
                    <span>Visual Grounding paper accepted to TIP</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2022/03</span>
                    <span>HOI Detection paper accepted to CVPR 2022</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/09</span>
                    <span>HOI Detection paper accepted to NeurIPS 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/06</span>
                    <span>1st in CVPR2021 ActivityNet Homage challenge</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/03</span>
                    <span>HOI Detection paper accepted to CVPR 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/02</span>
                    <span>PIC Workshop at CVPR 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2020/02</span>
                    <span>Three papers accepted to CVPR 2020</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2019/10</span>
                    <span>Co-organizer of PIC Workshop at ICCV 2019</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2018/09</span>
                    <span>Co-organizer of PIC Workshop at ECCV 2018</span>
                </li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Publications</h2>
            <div class="publication">
                <h3 class="pub-title">LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</h3>
                <p class="pub-authors">Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, et al.</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2408.15881"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/shufangxun/LLaVA-MoD"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More</h3>
                <p class="pub-authors">Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, et al.</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2410.06270"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/Aaronhuang-778/Mixture-Compressor-MoE"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</h3>
                <p class="pub-authors">Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, et al.</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2410.07087"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://prince687028.github.io/OpenUAV"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Anchor3DLane++: 3D Lane Detection via Sample-Adaptive Sparse 3D Anchor Regression</h3>
                <p class="pub-authors">Shaofei Huang, Zhenwei Shen, Zehao Huang, Yue Liao, etc.</p>
                <p class="pub-venue">IEEE TPAMI 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2412.16889"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/tusen-ai/Anchor3DLane"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction</h3>
                <p class="pub-authors">Penghui Du, Yu Wang, Yifan Sun, Luting Wang, Yue Liao, etc.</p>
                <p class="pub-venue">ECCV 2024</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2407.11335"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/eternaldolphin/LaMI-DETR"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation</h3>
                <p class="pub-authors">Hairong Shi, Songhao Han, Shaofei Huang, Yue Liao, etc.</p>
                <p class="pub-venue">MICCAI 2024</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1007/978-3-031-72111-3_38"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/nanase1025/M-SAM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">PPDM++: Parallel Point Detection and Matching for Fast and Accurate HOI Detection</h3>
                <p class="pub-authors">Yue Liao, Si Liu, Yulu Gao, Aixi Zhang, etc.</p>
                <p class="pub-venue">IEEE TPAMI 2024</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/PAMI_PPDM_final.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/PPDM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">MAC: Masked Contrastive Pre-Training for Efficient Video-Text Retrieval</h3>
                <p class="pub-authors">Fangxun Shu, Biaolong Chen, Yue Liao, Jinqiao Wang, etc.</p>
                <p class="pub-venue">IEEE TMM 2024</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2212.00986"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</h3>
                <p class="pub-authors">Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, etc.</p>
                <p class="pub-venue">ACM MM 2023</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2308.02915"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Video Background Music Generation: Dataset, Method and Evaluation</h3>
                <p class="pub-authors">Le Zhuo, Zhaokai Wang, Yue Liao, etc.</p>
                <p class="pub-venue">ICCV 2023</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/zhuole1025/SymMV"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection</h3>
                <p class="pub-authors">Luting Wang, Yi Liu, Yue Liao, etc.</p>
                <p class="pub-venue">CVPR 2023</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/LutingWang/OADP"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Simultaneously Training and Compressing Vision-and-Language Pre-Training Model</h3>
                <p class="pub-authors">Qiaosong Qi, Aixi Zhang, Yue Liao, etc.</p>
                <p class="pub-venue">IEEE TMM 2023</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/TMM.2022.3233258"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">HEAD: Hetero-Assists Distillation for Heterogeneous Object Detectors</h3>
                <p class="pub-authors">Luting Wang, Xiaojie Li, Yue Liao, etc.</p>
                <p class="pub-venue">ECCV 2022</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2207.05345"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/LutingWang/HEAD"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection</h3>
                <p class="pub-authors">Yue Liao, Aixi Zhang, Miao Lu, etc.</p>
                <p class="pub-venue">CVPR 2022</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_GEN-VLKT_Simplify_Association_and_Enhance_Interaction_Understanding_for_HOI_Detection_CVPR_2022_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/gen-vlkt"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Human-Centric Relation Segmentation: Dataset and Solution</h3>
                <p class="pub-authors">Si Liu, Zitian Wang, Yue Liao, etc.</p>
                <p class="pub-venue">IEEE TPAMI 2022</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/Human-centric_Relation_Segmentation_Dataset_and_Solution_jBmLebt.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://intxyz-my.sharepoint.com/:f:/g/personal/zongheng_picdataset_com/Eqmkhfe9qHxHoFCoXxAxYpYBe6aiztxvPQXuj3j9DWkkrg?e=9LGt5s"><i class="fas fa-cloud"></i> Dataset</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Mining the Benefits of Two-stage and One-stage HOI Detection</h3>
                <p class="pub-authors">Aixi Zhang, Yue Liao, Si Liu, etc.</p>
                <p class="pub-venue">NeurIPS 2021</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2108.05077"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/CDN"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Progressive Language-Customized Visual Feature Learning for One-Stage Visual Grounding</h3>
                <p class="pub-authors">Yue Liao, Aixi Zhang, Zhiyuan Chen, etc.</p>
                <p class="pub-venue">IEEE TIP 2022</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/Progressive_Language-Customized_Visual_Feature_Learning_for_One-Stage_Visual_Grounding.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Reformulating HOI Detection as Adaptive Set Prediction</h3>
                <p class="pub-authors">Mingfei Chen, Yue Liao, Si Liu, etc.</p>
                <p class="pub-venue">CVPR 2021</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Reformulating_HOI_Detection_As_Adaptive_Set_Prediction_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/yoyomimi/AS-Net"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Human-Centric Spatio-Temporal Video Grounding With Visual Transformers</h3>
                <p class="pub-authors">Zongheng Tang, Yue Liao, Si Liu, etc.</p>
                <p class="pub-venue">IEEE TCSVT 2021</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2011.05049"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/tzhhhh123/HC-STVG"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Scene Graph Generation With Hierarchical Context</h3>
                <p class="pub-authors">Guanghui Ren, Lejian Ren, Yue Liao, etc.</p>
                <p class="pub-venue">IEEE TNNLS 2021</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/TNNLS.2020.2979270"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Cross-Modal Omni Interaction Modeling for Phrase Grounding</h3>
                <p class="pub-authors">Tianyu Yu, Tianrui Hui, Yue Liao, etc.</p>
                <p class="pub-venue">ACM MM 2020</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1145/3394171.3413846"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/yiranyyu/Phrase-Grounding"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Local Correlation Consistency for Knowledge Distillation</h3>
                <p class="pub-authors">Xiaojie Li, Jianlong Wu, Yue Liao, etc.</p>
                <p class="pub-venue">ECCV 2020</p>
                <div class="pub-links">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">PPDM: Parallel Point Detection and Matching for Real-time Human-Object Interaction Detection</h3>
                <p class="pub-authors">Yue Liao, Si Liu, Fei Wang, etc.</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_PPDM_Parallel_Point_Detection_and_Matching_for_Real-Time_Human-Object_Interaction_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/PPDM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension</h3>
                <p class="pub-authors">Yue Liao, Si Liu, Guanbin Li, etc.</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression_Comprehension_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection</h3>
                <p class="pub-authors">Zhiwei Dong, Guoxuan Li, Yue Liao, etc.</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_CentripetalNet_Pursuing_High-Quality_Keypoint_Pairs_for_Object_Detection_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/KiveeDong/CentripetalNet"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">GPS: Group People Segmentation with Detailed Part Inference</h3>
                <p class="pub-authors">Yue Liao, Si Liu, Tianrui Hui, etc.</p>
                <p class="pub-venue">ICME 2019</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/ICME.2019.00112"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-heading">Academic Services</h2>
            <ul class="list">
                <li>Workshop Organizer: Person in Context (PIC) workshops at ECCV 2018, ICCV 2019, CVPR 2021</li>
                <li>Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR</li>
                <li>Journal Reviewer: TPAMI, TIP, TMM, TNNLS</li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Awards</h2>
            <ul class="list">
                <li>The First Prize of Natural Science Award (CSIG) - 2023</li>
                <li>Huawei Scholarship of Beihang University - 2023</li>
                <li>National Scholarship - 2022</li>
                <li>Alibaba Outstanding Intern Award - 2022</li>
                <li>CVPR 2021 ActivityNet Homage Challenge 1st Place - 2021</li>
                <li>Sensetime "Future Star" Award - 2020</li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Personal Interests</h2>
            <p>Sports (Tennis, Basketball, Hiking), Travel</p>
        </section>
    </div>
</body>
</html>

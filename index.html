<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yue Liao | NUS</title>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #1772d0;
            --accent-color: #e74c3c;
            --text-color: #34495e;
            --bg-light: #f9f9f9;
            --shadow: 0 2px 15px rgba(0,0,0,0.1);
        }

        body {
            font-family: 'Lato', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 20px;
            background-color: white;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .header {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            align-items: center;
            padding: 40px 0;
            border-bottom: 2px solid #eee;
        }

        .profile-img {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            object-fit: cover;
            box-shadow: var(--shadow);
        }

        .name {
            font-size: 2.5em;
            margin: 0;
            color: var(--primary-color);
        }

        .contact-links {
            margin-top: 15px;
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }

        .contact-links a {
            color: var(--secondary-color);
            transition: color 0.3s;
            text-decoration: none;
        }

        .contact-links a:hover {
            color: var(--accent-color);
        }

        .section {
            margin: 40px 0;
            padding: 30px;
            background: white;
            border-radius: 10px;
            box-shadow: var(--shadow);
        }

        .section-heading {
            font-size: 1.8em;
            color: var(--primary-color);
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
            margin: 0 0 25px 0;
        }

        .news-list {
            list-style: none;
            padding: 0;
        }

        .news-item {
            padding: 15px 0;
            border-bottom: 1px solid #eee;
            display: flex;
            align-items: baseline;
        }

        .news-date {
            color: var(--accent-color);
            min-width: 100px;
            font-weight: 700;
        }

        .publication {
            padding: 20px;
            margin: 15px 0;
            background: var(--bg-light);
            border-radius: 8px;
            transition: transform 0.2s;
        }

        .publication:hover {
            transform: translateY(-3px);
        }

        .pub-title {
            font-size: 1.1em;
            color: var(--primary-color);
            margin: 0 0 10px 0;
        }

        .pub-authors {
            color: #666;
            margin: 5px 0;
        }

        .pub-venue {
            color: var(--secondary-color);
            font-weight: 700;
        }

        .pub-links {
            margin-top: 10px;
        }

        .pub-links a {
            margin-right: 15px;
            color: var(--accent-color);
            text-decoration: none;
        }

        .list {
            list-style: none;
            padding: 0;
        }

        .list li {
            padding: 8px 0;
            border-bottom: 1px solid #eee;
        }

        @media (max-width: 768px) {
            .header {
                flex-direction: column;
                text-align: center;
            }
            
            .contact-links {
                justify-content: center;
            }
            
            .section {
                padding: 20px;
            }
            
            .news-date {
                min-width: 80px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="header-text">
                <h1 class="name">Yue Liao <span style="font-family: 'KaiTi', sans-serif; font-size: 0.8em;">廖越</span></h1>
                <p>Research Fellow<br> National University of Singapore</p>
                <div class="contact-links">
                    <a href="mailto:liaoyue.ai@gmail.com"><i class="fas fa-envelope"></i> Email</a>
                    <a href="https://scholar.google.com/citations?user=mIt-3fEAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Scholar</a>
                    <a href="https://github.com/YueLiao"><i class="fab fa-github"></i> GitHub</a>
                </div>
            </div>
            <img src="images/liaoyue.jpg" alt="Yue Liao" class="profile-img">
        </header>

        <section class="section">
            <h2 class="section-heading">Bio</h2>
            <p>I am currently a Research Fellow at the LV-Lab, National University of Singapore, working under the supervision of Prof. Shuicheng Yan. Prior to this, I spent a wonderful year as a Postdoctoral Fellow at MMLab, The Chinese University of Hong Kong, under the guidance of Prof.  <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>. I completed my PhD at Beihang University (BUAA) in 2023, where I was supervised by Prof. <a href="http://colalab.net/people">Si Liu</a>. Prior to my PhD, I gained valuable industry experience as a Research Intern at Alibaba Group and SenseTime Ltd. I pursued my Master's degree at the University of Chinese Academy of Sciences (UCAS), under the guidance of Prof. Si Liu. I obtained my Bachelor's degree from Northeastern University, China.
                <p>
                  My research interests include Multi-modality understanding and Embodied AI.</p>
        </section>

        <section class="section">
            <h2 class="section-heading">Latest News</h2>
            <ul class="news-list">
                <li class="news-item">
                    <span class="news-date">2025/05</span>
                    <span>Two co-authored papers were accepted by Advanced Science and IEEE RAL, respectively.</span>
                </li>
                 <li class="news-item">
                    <span class="news-date">2025/02</span>
                    <span>One paper for Video CoT accepted to CVPR 2025  selected as Oral</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2025/01</span>
                    <span>Three papers accepted to ICLR 2025</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2022/06</span>
                    <span>A paper on Visual Grounding was accepted to TIP</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2022/03</span>
                    <span>A paper on HOI Detection was accepted to CVPR 2022</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/09</span>
                    <span>A paper on HOI Detection was accepted to NeurIPS 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/06</span>
                    <span>1st place in the CVPR2021 ActivityNet Homage challenge</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/03</span>
                    <span>A paper on HOI Detection was accepted to CVPR 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2021/02</span>
                    <span>The 3rd Person in Context (PIC) Workshop will be held at CVPR 2021</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2020/02</span>
                    <span>Three papers were accepted to CVPR 2020</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2019/10</span>
                    <span>I am co-organizing the 2nd Person in Context (PIC) Workshop at ICCV 2019</span>
                </li>
                <li class="news-item">
                    <span class="news-date">2018/09</span>
                    <span>I am a co-organizer of the Person in Context (PIC) Workshop at ECCV 2018</span>
                </li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Publications</h2>
            <div class="publication">
                <h3 class="pub-title">Perovskite Neuromorphic Engine for Transformer Architectures</h3>
                <p class="pub-authors">Zhenye Zhan, Yulu Gao, <strong>Yue Liao</strong>, Weiguang Xie, Si Liu, Xiaomu Wang</p>
                <p class="pub-venue">Advanced Science</p>
                <div class="pub-links">
                    <a href=""><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Contrastive Learning-Based Secure Unsupervised Domain Adaptation Framework and Its Application in Cross-Factory Intelligent Manufacturing</h3>
                <p class="pub-authors">Zeyi Liu, Weihua Gui, Keke Huang, Dehao Wu, <strong>Yue Liao</strong>, Chunhua Yang</p>
                <p class="pub-venue">RA-L</p>
                <div class="pub-links">
                    <a href="https://ieeexplore.ieee.org/abstract/document/10948317/"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</h3>
                <p class="pub-authors">Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, <strong>Yue Liao†</strong>, Si Liu 
</p>
                <p class="pub-venue">CVPR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2411.14794"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/hshjerry/VideoEspresso"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</h3>
                <p class="pub-authors">Fangxun Shu*, <strong>Yue Liao*</strong>, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu, Hongsheng Li, Hao Jiang</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2408.15881"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/shufangxun/LLaVA-MoD"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More</h3>
                <p class="pub-authors">Wei Huang*, <strong>Yue Liao*</strong>, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li, Si Liu, Xiaojuan Qi</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2410.06270"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/Aaronhuang-778/Mixture-Compressor-MoE"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology</h3>
                <p class="pub-authors">Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, <strong>Yue Liao†</strong>, Si Liu</p>
                <p class="pub-venue">ICLR 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2410.07087"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://prince687028.github.io/OpenUAV"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Anchor3DLane++: 3D Lane Detection via Sample-Adaptive Sparse 3D Anchor Regression</h3>
                <p class="pub-authors">Shaofei Huang, Zhenwei Shen, Zehao Huang, <strong>Yue Liao</strong>, Jizhong Han, Naiyan Wang, Si Liu</p>
                <p class="pub-venue">IEEE TPAMI 2025</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2412.16889"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/tusen-ai/Anchor3DLane"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction</h3>
                <p class="pub-authors">Penghui Du, Yu Wang, Yifan Sun, Luting Wang, <strong>Yue Liao</strong>, Gang Zhang, Errui Ding, Yan Wang, Jingdong Wang, Si Liu</p>
                <p class="pub-venue">ECCV 2024</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2407.11335"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/eternaldolphin/LaMI-DETR"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation</h3>
                <p class="pub-authors">Hairong Shi, Songhao Han, Shaofei Huang, <strong>Yue Liao</strong>, Guanbin Li, Xiangxing Kong, Hua Zhu, Xiaomu Wang, Si Liu</p>
                <p class="pub-venue">MICCAI 2024</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1007/978-3-031-72111-3_38"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/nanase1025/M-SAM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">PPDM++: Parallel Point Detection and Matching for Fast and Accurate HOI Detection</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Si Liu, Yulu Gao, Aixi Zhang, Zhimin Li, Fei Wang, Bo Li</p>
                <p class="pub-venue">IEEE TPAMI 2024</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/PAMI_PPDM_final.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/PPDM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">MAC: Masked Contrastive Pre-Training for Efficient Video-Text Retrieval</h3>
                <p class="pub-authors">Fangxun Shu, Biaolong Chen, <strong>Yue Liao†</strong>, Jinqiao Wang, Si Liu</p>
                <p class="pub-venue">IEEE TMM 2024</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2212.00986"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</h3>
                <p class="pub-authors">Qiaosong Qi, Le Zhuo, Aixi Zhang, <strong>Yue Liao</strong>, Fei Fang, Si Liu, Shuicheng Yan</p>
                <p class="pub-venue">ACM MM 2023</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2308.02915"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Video Background Music Generation: Dataset, Method and Evaluation</h3>
                <p class="pub-authors">Le Zhuo, Zhaokai Wang, Baisen Wang, <strong>Yue Liao†</strong>, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</p>
                <p class="pub-venue">ICCV 2023</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhuo_Video_Background_Music_Generation_Dataset_Method_and_Evaluation_ICCV_2023_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/zhuole1025/SymMV"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection</h3>
                <p class="pub-authors">Luting Wang, Yi Liu, Penghui Du, Zihan Ding, <strong>Yue Liao†</strong>, Qiaosong Qi, Biaolong Chen, Si Liu</p>
                <p class="pub-venue">CVPR 2023</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/LutingWang/OADP"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Simultaneously Training and Compressing Vision-and-Language Pre-Training Model</h3>
                <p class="pub-authors">Qiaosong Qi, Aixi Zhang, <strong>Yue Liao†</strong>, Wenyu Sun, Yongliang Wang, Xiaobo Li, Si Liu</p>
                <p class="pub-venue">IEEE TMM 2023</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/TMM.2022.3233258"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">HEAD: Hetero-Assists Distillation for Heterogeneous Object Detectors</h3>
                <p class="pub-authors">Luting Wang, Xiaojie Li, <strong>Yue Liao†</strong>, Zeren Jiang, Jianlong Wu, Fei Wang, Chen Qian, Si Liu</p>
                <p class="pub-venue">ECCV 2022</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2207.05345"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/LutingWang/HEAD"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, Si Liu</p>
                <p class="pub-venue">CVPR 2022</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_GEN-VLKT_Simplify_Association_and_Enhance_Interaction_Understanding_for_HOI_Detection_CVPR_2022_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/gen-vlkt"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Human-Centric Relation Segmentation: Dataset and Solution</h3>
                <p class="pub-authors">Si Liu, Zitian Wang, Yulu Gao, Lejian Ren, <strong>Yue Liao</strong>, Guanghui Ren, Bo Li, Shuicheng Yan</p>
                <p class="pub-venue">IEEE TPAMI 2022</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/Human-centric_Relation_Segmentation_Dataset_and_Solution_jBmLebt.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://intxyz-my.sharepoint.com/:f:/g/personal/zongheng_picdataset_com/Eqmkhfe9qHxHoFCoXxAxYpYBe6aiztxvPQXuj3j9DWkkrg?e=9LGt5s"><i class="fas fa-cloud"></i> Dataset</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Mining the Benefits of Two-stage and One-stage HOI Detection</h3>
                <p class="pub-authors">Aixi Zhang*, <strong>Yue Liao*</strong>, Si Liu, Miao Lu, Yongliang Wang, Chen Gao, Xiaobo Li</p>
                <p class="pub-venue">NeurIPS 2021</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2108.05077"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/CDN"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Progressive Language-Customized Visual Feature Learning for One-Stage Visual Grounding</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Aixi Zhang, Zhiyuan Chen, Tianrui Hui, Si Liu</p>
                <p class="pub-venue">IEEE TIP 2022</p>
                <div class="pub-links">
                    <a href="https://colalab.net/media/paper/Progressive_Language-Customized_Visual_Feature_Learning_for_One-Stage_Visual_Grounding.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Reformulating HOI Detection as Adaptive Set Prediction</h3>
                <p class="pub-authors">Mingfei Chen*, <strong>Yue Liao*</strong>, Si Liu, Zhiyuan Chen, Fei Wang, Chen Qian</p>
                <p class="pub-venue">CVPR 2021</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Reformulating_HOI_Detection_As_Adaptive_Set_Prediction_CVPR_2021_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/yoyomimi/AS-Net"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Human-Centric Spatio-Temporal Video Grounding With Visual Transformers</h3>
                <p class="pub-authors">Zongheng Tang, <strong>Yue Liao</strong>, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu</p>
                <p class="pub-venue">IEEE TCSVT 2021</p>
                <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2011.05049"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/tzhhhh123/HC-STVG"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Scene Graph Generation With Hierarchical Context</h3>
                <p class="pub-authors">Guanghui Ren, Lejian Ren, <strong>Yue Liao</strong>, Si Liu, Bo Li, Jizhong Han, Shuicheng Yan</p>
                <p class="pub-venue">IEEE TNNLS 2021</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/TNNLS.2020.2979270"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Cross-Modal Omni Interaction Modeling for Phrase Grounding</h3>
                <p class="pub-authors">Tianyu Yu, Tianrui Hui, Zhihao Yu, <strong>Yue Liao</strong>, Sansi Yu, Faxi Zhang, Si Liu</p>
                <p class="pub-venue">ACM MM 2020</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1145/3394171.3413846"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/yiranyyu/Phrase-Grounding"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">Local Correlation Consistency for Knowledge Distillation</h3>
                <p class="pub-authors">Xiaojie Li, Jianlong Wu, Hongyu Fang, <strong>Yue Liao</strong>, Fei Wang, Chen Qian</p>
                <p class="pub-venue">ECCV 2020</p>
                <div class="pub-links">
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">PPDM: Parallel Point Detection and Matching for Real-time Human-Object Interaction Detection</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, Jiashi Feng</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_PPDM_Parallel_Point_Detection_and_Matching_for_Real-Time_Human-Object_Interaction_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/YueLiao/PPDM"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liao_A_Real-Time_Cross-Modality_Correlation_Filtering_Method_for_Referring_Expression_Comprehension_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection</h3>
                <p class="pub-authors">Zhiwei Dong, Guoxuan Li, <strong>Yue Liao</strong>, Fei Wang, Pengju Ren, Chen Qian</p>
                <p class="pub-venue">CVPR 2020</p>
                <div class="pub-links">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_CentripetalNet_Pursuing_High-Quality_Keypoint_Pairs_for_Object_Detection_CVPR_2020_paper.pdf"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/KiveeDong/CentripetalNet"><i class="fas fa-code"></i> Code</a>
                </div>
            </div>
            <div class="publication">
                <h3 class="pub-title">GPS: Group People Segmentation with Detailed Part Inference</h3>
                <p class="pub-authors"><strong>Yue Liao</strong>, Si Liu, Tianrui Hui, Chen Gao, Yao Sun, Hefei Ling, Bo Li</p>
                <p class="pub-venue">ICME 2019</p>
                <div class="pub-links">
                    <a href="https://doi.org/10.1109/ICME.2019.00112"><i class="fas fa-file-pdf"></i> Paper</a>
                </div>
            </div>
        </section>

        <section class="section">
            <h2 class="section-heading">Academic Services</h2>
            <ul class="list">
                <li>Workshop Organizer: Person in Context (PIC) workshops at ECCV 2018, ICCV 2019, and CVPR 2021</li>
                <li>Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI, IJCAI, ACM MM </li>
                <li>Journal Reviewer: TPAMI, TIP, TMM, TNNLS, TCSTV, ACM CSUR</li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Awards</h2>
            <ul class="list">
                <li>The First Prize of the Natural Science Award of the China Society for Image and Graphics (CSIG) <em>2023</em></li>
                <li>The Huawei Scholarship of Beihang University <em>2023</em></li>
                <li>National Scholarship <em>2022</em></li>
                <li>Alibaba 'Outstanding Academic Cooperation and Research Intern' Award <em>2022</em></li>
                <li>The Champion of ActivityNet Homage challenge (CVPR) <em>2021</em></li>
                <li>Sensetime Co., Ltd. 'Future Star' Award <em>2020</em></li>
            </ul>
        </section>

        <section class="section">
            <h2 class="section-heading">Personal Interests</h2>
            <p>I avidly pursue sports, particularly tennis, basketball, and hiking, with a special admiration for iconic figures like Roger Federer and Kobe Bryant. Traveling is another passion of mine, as it broadens my horizons and deepens my understanding of diverse cultures and lifestyles.</p>
        </section>
    </div>
</body>
</html>
